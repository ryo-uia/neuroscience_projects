{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a2948b3",
      "metadata": {},
      "source": [
        "# SC2 Post-sort QC\n",
        "\n",
        "Supports **tetrode** and **mixed** pipelines. Set `PIPELINE_MODE` in the imports cell.\n",
        "Suggested run order: top to bottom; optional sections are marked.\n",
        "Group-level diagnostics run when `RUN_TETRODE_CHECKS` is True (default: enabled for both group and mixed modes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57f6d2d",
      "metadata": {},
      "source": [
        "### Imports and setup\n",
        "Run the next code cell first to initialize imports, paths, and notebook defaults.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7f3643",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and shared defaults (run first).\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import json\n",
        "import runpy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import spikeinterface.widgets as sw\n",
        "\n",
        "# Pipeline mode.\n",
        "PIPELINE_MODE = 'tetrode'  # 'tetrode' or 'mixed'\n",
        "FORCE_TETRODE_CHECKS = None  # set True/False to override auto mode\n",
        "SKIP_NOISY_CELLS = False  # set True to skip heavy/plotting cells\n",
        "# This gate controls notebook diagnostics/plots only; it does not change pipeline sorting behavior.\n",
        "RUN_TETRODE_CHECKS = True if FORCE_TETRODE_CHECKS is None else FORCE_TETRODE_CHECKS\n",
        "\n",
        "\n",
        "# Optional deps (safe if missing).\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "# User inputs and defaults.\n",
        "# Resolve project root for notebook execution context.\n",
        "cwd = Path.cwd().resolve()\n",
        "if (cwd / 'SpikeSorting').exists():\n",
        "    PROJECT_ROOT = cwd\n",
        "elif cwd.name == 'SpikeSorting':\n",
        "    PROJECT_ROOT = cwd.parent\n",
        "elif cwd.name == 'Analysis' and cwd.parent.name == 'SpikeSorting':\n",
        "    PROJECT_ROOT = cwd.parent.parent\n",
        "else:\n",
        "    PROJECT_ROOT = cwd\n",
        "base_spike = PROJECT_ROOT / 'SpikeSorting' if (PROJECT_ROOT / 'SpikeSorting').exists() else PROJECT_ROOT\n",
        "SC2_OUT = base_spike / 'sc2_outputs'\n",
        "phy_folder = None  # set to a specific phy export folder\n",
        "data_path = None   # Open Ephys folder (for mapping / raw comparisons)\n",
        "stream_name = 'auto'\n",
        "oe_pick_index = None\n",
        "OE_PROMPT_SELECT = True\n",
        "EXPORTED_CHANNEL_LABELS = 'as_exported'  # oe_index | oe_label | as_exported\n",
        "\n",
        "# Keep synthetic geometry previews aligned with pipeline defaults.\n",
        "TETRODE_SPACING_DX_UM = 300.0\n",
        "TETRODE_SPACING_DY_UM = 300.0\n",
        "TETRODE_PITCH_UM = 20.0\n",
        "\n",
        "# Placeholders to avoid NameError later.\n",
        "analyzer = None\n",
        "sorting = None\n",
        "\n",
        "def _require(value, name: str):\n",
        "    if value is None:\n",
        "        raise ValueError(f'{name} is not set. Update the config cell above and re-run.')\n",
        "    return value\n",
        "\n",
        "def get_traces_scaled(recording, start_frame=0, end_frame=None):\n",
        "    traces = recording.get_traces(start_frame=start_frame, end_frame=end_frame)\n",
        "    try:\n",
        "        gains = recording.get_channel_gains()\n",
        "        offsets = recording.get_channel_offsets()\n",
        "        if gains is not None:\n",
        "            traces = traces * np.asarray(gains)[None, :]\n",
        "        if offsets is not None:\n",
        "            traces = traces + np.asarray(offsets)[None, :]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return traces\n",
        "\n",
        "def _load_phy_geometry(phy_folder):\n",
        "    pos_path = phy_folder / 'channel_positions.npy'\n",
        "    if not pos_path.exists():\n",
        "        pos_path = phy_folder / 'channel_locations.npy'\n",
        "    positions = np.load(pos_path) if pos_path.exists() else None\n",
        "\n",
        "    labels = None\n",
        "    ch_ids_path = phy_folder / 'channel_ids.npy'\n",
        "    if ch_ids_path.exists():\n",
        "        try:\n",
        "            labels = np.load(ch_ids_path)\n",
        "        except Exception:\n",
        "            labels = None\n",
        "\n",
        "    groups = None\n",
        "    groups_path = phy_folder / 'channel_groups.npy'\n",
        "    if groups_path.exists():\n",
        "        try:\n",
        "            groups = np.load(groups_path)\n",
        "        except Exception:\n",
        "            groups = None\n",
        "\n",
        "    return positions, labels, groups, pos_path\n",
        "\n",
        "\n",
        "def _build_synthetic_positions(channel_groups, dx, dy, pitch):\n",
        "    if channel_groups is None:\n",
        "        return None\n",
        "    unique_groups = sorted({int(g) for g in channel_groups.tolist()})\n",
        "    if not unique_groups:\n",
        "        return None\n",
        "    groups_per_row = max(1, int(np.ceil(np.sqrt(len(unique_groups)))))\n",
        "    positions = np.zeros((channel_groups.size, 2), dtype=float)\n",
        "    base = np.array([[0.0, 0.0], [pitch, 0.0], [0.0, pitch], [pitch, pitch]], dtype=float)\n",
        "    for g_idx, group in enumerate(unique_groups):\n",
        "        row = g_idx // groups_per_row\n",
        "        col = g_idx % groups_per_row\n",
        "        offset = np.array([col * dx, (len(unique_groups) - 1 - row) * dy], dtype=float)\n",
        "        idxs = np.where(channel_groups == group)[0]\n",
        "        # Plot-only fallback geometry for readability; does not affect sorter geometry or outputs.\n",
        "        for slot, ch_i in enumerate(idxs):\n",
        "            positions[ch_i] = offset + base[slot % base.shape[0]]\n",
        "    return positions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8213b716",
      "metadata": {},
      "source": [
        "## Quick status check (mode + flags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d368544",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'PIPELINE_MODE={PIPELINE_MODE} | RUN_TETRODE_CHECKS={RUN_TETRODE_CHECKS} | SKIP_NOISY_CELLS={SKIP_NOISY_CELLS}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0de6824",
      "metadata": {},
      "source": [
        "### Select Phy export and Open Ephys root\n",
        "Expected: a selected `phy_folder` path and (optionally) an OE root.\n",
        "Note: Open Ephys helpers are defined in the resolve cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25ba4ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a Phy export folder (uses defaults above).\n",
        "# Ensure SC2_OUT points at the real sc2_outputs folder if run from a nested cwd.\n",
        "sc2_candidates = []\n",
        "try:\n",
        "    sc2_candidates.append(SC2_OUT)\n",
        "except Exception:\n",
        "    SC2_OUT = None\n",
        "\n",
        "if SC2_OUT is None or not Path(SC2_OUT).exists():\n",
        "    sc2_candidates = []\n",
        "\n",
        "base = PROJECT_ROOT\n",
        "sc2_candidates += [\n",
        "    base / \"SpikeSorting\" / \"sc2_outputs\",\n",
        "    base / \"sc2_outputs\",\n",
        "    base.parent / \"SpikeSorting\" / \"sc2_outputs\",\n",
        "]\n",
        "for cand in sc2_candidates:\n",
        "    if cand and Path(cand).exists():\n",
        "        SC2_OUT = Path(cand)\n",
        "        break\n",
        "\n",
        "def find_phy_dirs(base: Path):\n",
        "    if not base.exists():\n",
        "        return []\n",
        "    return sorted(\n",
        "        [p for p in base.glob('phy_*') if (p / 'channel_ids.npy').exists()],\n",
        "        key=lambda p: p.stat().st_mtime,\n",
        "        reverse=True,\n",
        "    )\n",
        "\n",
        "pick_index = None  # set to an int to pick a specific export from sorted list\n",
        "PHY_PROMPT_SELECT = True  # prompt when multiple phy_* exports are found\n",
        "\n",
        "if phy_folder is None:\n",
        "    dirs = find_phy_dirs(SC2_OUT)\n",
        "    if not dirs:\n",
        "        raise RuntimeError(f'No phy_* exports found under {SC2_OUT}')\n",
        "    if len(dirs) > 1 and pick_index is None and PHY_PROMPT_SELECT:\n",
        "        print('Available phy exports:')\n",
        "        for idx, path in enumerate(dirs):\n",
        "            print('  [{}] {}'.format(idx, path))\n",
        "        choice = input('Select phy export index [default 0]: ').strip()\n",
        "        if choice:\n",
        "            try:\n",
        "                pick_index = int(choice)\n",
        "            except ValueError:\n",
        "                print('Invalid selection; using [0]')\n",
        "        else:\n",
        "            pick_index = 0\n",
        "    if pick_index is None:\n",
        "        phy_folder = dirs[0]\n",
        "    else:\n",
        "        if 0 <= pick_index < len(dirs):\n",
        "            phy_folder = dirs[pick_index]\n",
        "        else:\n",
        "            print(f'pick_index {pick_index} out of range; using [0]')\n",
        "            phy_folder = dirs[0]\n",
        "\n",
        "print('Using phy export:', phy_folder)\n",
        "# Optional: Open Ephys folder for label/gain checks.\n",
        "if data_path is None:\n",
        "    data_path = PROJECT_ROOT / 'SpikeSorting' / 'recordings'\n",
        "if stream_name == 'auto':\n",
        "    stream_name = None\n",
        "\n",
        "# Channel label preference for map checks (optional).\n",
        "if EXPORTED_CHANNEL_LABELS is None:\n",
        "    EXPORTED_CHANNEL_LABELS = 'as_exported'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7933af",
      "metadata": {},
      "source": [
        "### Resolve Open Ephys folder\n",
        "Expected: a chosen Open Ephys folder and available stream names (if detected).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "036a4bbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resolve Open Ephys folder.\n",
        "from spikeinterface.extractors import read_openephys\n",
        "from spikeinterface.extractors.extractor_classes import OpenEphysBinaryRecordingExtractor\n",
        "\n",
        "def find_oe_recordings(root: Path):\n",
        "    if not root.exists():\n",
        "        return []\n",
        "    if (root / 'structure.oebin').exists():\n",
        "        return [root]\n",
        "    return sorted({p.parent for p in root.rglob('structure.oebin')})\n",
        "\n",
        "def resolve_oe_folder(root: Path, pick_index=None, prompt=False):\n",
        "    candidates = find_oe_recordings(root)\n",
        "    if not candidates:\n",
        "        print(f'No Open Ephys recording folders found under {root}')\n",
        "        return None\n",
        "    if len(candidates) > 1:\n",
        "        print('Available Open Ephys folders:')\n",
        "        for idx, path in enumerate(candidates):\n",
        "            print(f'  [{idx}] {path}')\n",
        "        if pick_index is None and prompt:\n",
        "            print('Enter the index to select (blank = 0).')\n",
        "            sys.stdout.flush()\n",
        "            choice = input('Select Open Ephys folder index [default 0]: ').strip()\n",
        "            if choice:\n",
        "                try:\n",
        "                    pick_index = int(choice)\n",
        "                except ValueError:\n",
        "                    print('Invalid selection; using [0]')\n",
        "            else:\n",
        "                pick_index = 0\n",
        "    if pick_index is None:\n",
        "        return candidates[0]\n",
        "    if 0 <= pick_index < len(candidates):\n",
        "        return candidates[pick_index]\n",
        "    print(f'pick_index {pick_index} out of range; using [0]')\n",
        "    return candidates[0]\n",
        "\n",
        "def get_oe_recording(data_path, stream_name=None, oe_pick_index=None, prompt=False):\n",
        "    if data_path is None:\n",
        "        print('Set data_path to an Open Ephys folder to enable label checks.')\n",
        "        return None, None, stream_name, None, None\n",
        "    root = Path(data_path)\n",
        "    if not root.exists():\n",
        "        print(f'Open Ephys Root Not Found: {root}')\n",
        "        return None, None, stream_name, None, None\n",
        "\n",
        "    oe_folder = globals().get('oe_folder')\n",
        "    if oe_folder is None:\n",
        "        oe_folder = resolve_oe_folder(root, oe_pick_index, prompt)\n",
        "        if oe_folder:\n",
        "            print('Using Open Ephys folder:', oe_folder)\n",
        "    if oe_folder is None:\n",
        "        return None, None, stream_name, None, None\n",
        "\n",
        "    if stream_name is None:\n",
        "        try:\n",
        "            stream_names, _ = OpenEphysBinaryRecordingExtractor.get_streams(oe_folder)\n",
        "            print('Available streams:', stream_names)\n",
        "            if len(stream_names) == 1:\n",
        "                stream_name = stream_names[0]\n",
        "            else:\n",
        "                non_adc = [name for name in stream_names if 'ADC' not in name]\n",
        "                if non_adc:\n",
        "                    stream_name = non_adc[0]\n",
        "                    print(f'Auto-selected stream_name: {stream_name}')\n",
        "                else:\n",
        "                    print('Set stream_name explicitly to choose a stream.')\n",
        "        except Exception as exc:\n",
        "            print(f'Warning: could not list streams: {exc}')\n",
        "\n",
        "    recording = read_openephys(oe_folder, stream_name=stream_name)\n",
        "    oe_ids = list(recording.channel_ids)\n",
        "    try:\n",
        "        oe_names = list(recording.get_property('channel_name'))\n",
        "    except Exception:\n",
        "        oe_names = None\n",
        "    return recording, oe_folder, stream_name, oe_ids, oe_names\n",
        "\n",
        "def resolve_oe_label(raw_idx, oe_ids, oe_names):\n",
        "    try:\n",
        "        idx = int(raw_idx)\n",
        "    except (TypeError, ValueError):\n",
        "        return str(raw_idx)\n",
        "    if oe_names and 0 <= idx < len(oe_names):\n",
        "        return oe_names[idx]\n",
        "    if oe_ids and 0 <= idx < len(oe_ids):\n",
        "        return oe_ids[idx]\n",
        "    return '<out of range>'\n",
        "\n",
        "oe_folder = None\n",
        "if data_path and data_path.exists():\n",
        "    oe_folder = resolve_oe_folder(data_path, oe_pick_index, OE_PROMPT_SELECT)\n",
        "    if oe_folder:\n",
        "        print('Using Open Ephys folder:', oe_folder)\n",
        "else:\n",
        "    print('Set data_path to an Open Ephys folder to enable label checks.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0907067b",
      "metadata": {},
      "source": [
        "### Version snapshot\n",
        "Print key package versions for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78725210",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Version snapshot.\n",
        "print('python:', sys.version.split()[0])\n",
        "print('numpy:', np.__version__)\n",
        "try:\n",
        "    import spikeinterface as si\n",
        "    print('spikeinterface:', getattr(si, '__version__', 'unknown'))\n",
        "except Exception as exc:\n",
        "    print('spikeinterface: not available', exc)\n",
        "try:\n",
        "    import hdbscan\n",
        "    print('hdbscan:', getattr(hdbscan, '__version__', 'unknown'))\n",
        "except Exception as exc:\n",
        "    print('hdbscan: not available', exc)\n",
        "try:\n",
        "    import numba\n",
        "    print('numba:', getattr(numba, '__version__', 'unknown'))\n",
        "except Exception as exc:\n",
        "    print('numba: not available', exc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8dde9c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment checks.\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "phy_exe = shutil.which(\"phy\")\n",
        "if phy_exe:\n",
        "    try:\n",
        "        out = subprocess.check_output([phy_exe, \"--version\"], text=True).strip()\n",
        "        print(\"phy (CLI):\", out or \"available\")\n",
        "    except Exception as exc:\n",
        "        print(\"phy (CLI): found but failed:\", exc)\n",
        "else:\n",
        "    print(\"phy (CLI): not on PATH (use the newphy2 env if needed)\")\n",
        "\n",
        "try:\n",
        "    print(\"pandas:\", pd.__version__)\n",
        "except Exception:\n",
        "    print(\"pandas: not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc08aa3",
      "metadata": {},
      "source": [
        "### Export file sanity checks\n",
        "Note: `template_ind.npy` is optional in some exports.\n",
        "Expected: OK/MISSING status for required files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d4e286",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group-level diagnostics. Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Diagnostics Cell (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "    # Export file sanity checks.\n",
        "    required = [\n",
        "        'spike_times.npy',\n",
        "        'spike_clusters.npy',\n",
        "        'templates.npy',\n",
        "        'channel_ids.npy',\n",
        "        'channel_groups.npy',\n",
        "    ]\n",
        "    optional = [\n",
        "        'template_ind.npy',\n",
        "    ]\n",
        "\n",
        "    for name in required:\n",
        "        path = phy_folder / name\n",
        "        print(f'{name}:', 'OK' if path.exists() else 'MISSING')\n",
        "    for name in optional:\n",
        "        path = phy_folder / name\n",
        "        print(f'{name}:', 'OK (optional)' if path.exists() else 'missing (optional)')\n",
        "\n",
        "    if (phy_folder / 'spike_times.npy').exists() and (phy_folder / 'spike_clusters.npy').exists():\n",
        "        spike_times = np.load(phy_folder / 'spike_times.npy')\n",
        "        spike_clusters = np.load(phy_folder / 'spike_clusters.npy')\n",
        "        print('spike_times:', spike_times.shape, 'spike_clusters:', spike_clusters.shape)\n",
        "        if spike_times.shape[0] != spike_clusters.shape[0]:\n",
        "            print('Warning: spike_times and spike_clusters length mismatch')\n",
        "\n",
        "    if (phy_folder / 'cluster_info.tsv').exists() and (phy_folder / 'spike_clusters.npy').exists():\n",
        "        spike_clusters = np.load(phy_folder / 'spike_clusters.npy')\n",
        "        max_cluster = int(spike_clusters.max()) if spike_clusters.size else -1\n",
        "        print('max cluster id:', max_cluster)\n",
        "\n",
        "    params_path = phy_folder / 'params.py'\n",
        "    if params_path.exists():\n",
        "        text = params_path.read_text()\n",
        "        for key in ('n_channels', 'sample_rate', 'dtype'):\n",
        "            for line in text.splitlines():\n",
        "                if line.strip().startswith(key):\n",
        "                    print('params', line.strip())\n",
        "                    break\n",
        "    else:\n",
        "        print('params.py Not Found')\n",
        "\n",
        "    # recording.dat size check (uses params.py if available).\n",
        "    dat_path = phy_folder / 'recording.dat'\n",
        "    if params_path.exists() and dat_path.exists():\n",
        "        try:\n",
        "            params = {}\n",
        "            for line in params_path.read_text().splitlines():\n",
        "                if '=' in line:\n",
        "                    key, val = line.split('=', 1)\n",
        "                    params[key.strip()] = val.strip()\n",
        "            n_channels = int(params.get('n_channels_dat', params.get('n_channels', 0)))\n",
        "            sample_rate = float(params.get('sample_rate', params.get('sample_rate_dat', 0.0)))\n",
        "            dtype_str = params.get('dtype', 'float32').strip(\"'\")\n",
        "            dtype = np.dtype(dtype_str)\n",
        "            size_bytes = dat_path.stat().st_size\n",
        "            if n_channels > 0:\n",
        "                denom = n_channels * dtype.itemsize\n",
        "                n_samples = size_bytes // denom\n",
        "                remainder = size_bytes % denom\n",
        "                print(f'recording.dat size: {size_bytes} bytes -> {n_samples} samples @ {n_channels} ch ({dtype})')\n",
        "                if remainder:\n",
        "                    print(f'Warning: recording.dat size has {remainder} extra bytes (unexpected)')\n",
        "        except Exception as exc:\n",
        "            print(f'Warning: recording.dat size check failed: {exc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c21f98c",
      "metadata": {},
      "source": [
        "### Export integrity checks\n",
        "Validate alignment of spike_times/spike_clusters/spike_templates.\n",
        "Expected: matching array lengths (or warnings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372c1760",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export integrity checks.\n",
        "spike_times_path = phy_folder / 'spike_times.npy'\n",
        "spike_clusters_path = phy_folder / 'spike_clusters.npy'\n",
        "spike_templates_path = phy_folder / 'spike_templates.npy'\n",
        "templates_path = phy_folder / 'templates.npy'\n",
        "\n",
        "if spike_times_path.exists() and spike_clusters_path.exists() and spike_templates_path.exists():\n",
        "    spike_times = np.load(spike_times_path).reshape(-1)\n",
        "    spike_clusters = np.load(spike_clusters_path).reshape(-1)\n",
        "    spike_templates = np.load(spike_templates_path).reshape(-1)\n",
        "    print('spike_times:', spike_times.shape, 'spike_clusters:', spike_clusters.shape, 'spike_templates:', spike_templates.shape)\n",
        "    if not (spike_times.size == spike_clusters.size == spike_templates.size):\n",
        "        print('Warning: spike_times/spike_clusters/spike_templates length mismatch')\n",
        "\n",
        "    if templates_path.exists():\n",
        "        templates = np.load(templates_path, mmap_mode='r')\n",
        "        max_template = int(spike_templates.max()) if spike_templates.size else -1\n",
        "        if max_template >= templates.shape[0]:\n",
        "            print(f'Warning: spike_templates max {max_template} >= templates count {templates.shape[0]}')\n",
        "else:\n",
        "    print('Export integrity check skipped (missing spike_times/spike_clusters/spike_templates).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36072457",
      "metadata": {},
      "source": [
        "### Spike time bounds check\n",
        "Ensure spike times are within the exported recording length.\n",
        "Expected: max spike time within bounds (or warning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9565b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spike time bounds check.\n",
        "\n",
        "spike_times_path = phy_folder / 'spike_times.npy'\n",
        "if not spike_times_path.exists():\n",
        "    print('spike_times.npy Not Found; Run Export First.')\n",
        "elif not params_path.exists():\n",
        "    print('params.py Not Found; cannot validate spike times.')\n",
        "else:\n",
        "    params = runpy.run_path(params_path)\n",
        "    dat_path = Path(params.get('dat_path', '')) if params.get('dat_path') else None\n",
        "    n_channels = int(params.get('n_channels_dat', params.get('n_channels', 0)))\n",
        "    sample_rate = float(params.get('sample_rate', params.get('sample_rate_dat', 0.0)))\n",
        "    dtype = np.dtype(params.get('dtype', 'float32'))\n",
        "    if dat_path and dat_path.exists() and n_channels > 0:\n",
        "        size_bytes = dat_path.stat().st_size\n",
        "        n_samples = size_bytes // (n_channels * dtype.itemsize)\n",
        "        spike_times = np.load(spike_times_path).astype(int).ravel()\n",
        "        max_spike = int(spike_times.max()) if spike_times.size else -1\n",
        "        if max_spike >= n_samples:\n",
        "            print(f'Warning: spike_times max {max_spike} exceeds recording length {n_samples}')\n",
        "        else:\n",
        "            print(f'spike_times max {max_spike} within recording length {n_samples}')\n",
        "        if sample_rate > 0:\n",
        "            print(f'recording duration: {n_samples / sample_rate:.2f}s; max spike at {max_spike / sample_rate:.2f}s')\n",
        "    else:\n",
        "        print('Could Not Compute Recording Length (Missing dat_path or n_channels).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f1d5fa",
      "metadata": {},
      "source": [
        "### params.py contents\n",
        "Print params.py for quick inspection.\n",
        "Expected: the full params.py text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2772ee5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# params.py contents.\n",
        "params_path = phy_folder / 'params.py'\n",
        "if params_path.exists():\n",
        "    print(params_path.read_text())\n",
        "else:\n",
        "    print('params.py Not Found')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8214cc3e",
      "metadata": {},
      "source": [
        "### Channel IDs, map, and groups\n",
        "Expected: loaded arrays with dtype/shape and basic length checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14f764b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group-level diagnostics. Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Diagnostics Cell (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "    # Print channel_ids/map/groups.\n",
        "    def load_and_print(path: Path, name: str):\n",
        "        if path.exists():\n",
        "            arr = np.load(path)\n",
        "            print(f'{name}: dtype={arr.dtype}, shape={arr.shape}')\n",
        "            print(np.array2string(arr, separator=' ', max_line_width=120))\n",
        "        else:\n",
        "            print(f'{name}: Not Found At {path}')\n",
        "\n",
        "    load_and_print(phy_folder / 'channel_ids.npy', 'channel_ids')\n",
        "    load_and_print(phy_folder / 'channel_map.npy', 'channel_map')\n",
        "    load_and_print(phy_folder / 'channel_groups.npy', 'channel_groups')\n",
        "    load_and_print(phy_folder / 'channel_shanks.npy', 'channel_shanks')\n",
        "\n",
        "    tsv = phy_folder / 'cluster_channel_group.tsv'\n",
        "    if tsv.exists():\n",
        "        print(f'\\n{tsv} (first 10 lines):')\n",
        "        for line in tsv.read_text(encoding='utf-8').splitlines()[:10]:\n",
        "            print(f'  {line}')\n",
        "    else:\n",
        "        print('cluster_channel_group.tsv: Not Found')\n",
        "\n",
        "    # Channel length checks.\n",
        "    try:\n",
        "        ch_ids = np.load(phy_folder / 'channel_ids.npy') if (phy_folder / 'channel_ids.npy').exists() else None\n",
        "        ch_map = np.load(phy_folder / 'channel_map.npy') if (phy_folder / 'channel_map.npy').exists() else None\n",
        "        ch_groups = np.load(phy_folder / 'channel_groups.npy') if (phy_folder / 'channel_groups.npy').exists() else None\n",
        "        lengths = []\n",
        "        for name, arr in [('channel_ids', ch_ids), ('channel_map', ch_map), ('channel_groups', ch_groups)]:\n",
        "            if arr is not None:\n",
        "                lengths.append((name, int(arr.shape[0])))\n",
        "        if lengths:\n",
        "            print('channel array lengths:', lengths)\n",
        "            vals = {l for _, l in lengths}\n",
        "            if len(vals) > 1:\n",
        "                print('Warning: channel array lengths differ')\n",
        "    except Exception as exc:\n",
        "        print(f'Warning: channel length check failed: {exc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350b4462",
      "metadata": {},
      "source": [
        "### Channel geometry preview\n",
        "Plot channel positions if available.\n",
        "Expected: a geometry plot (or a message if missing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08f5bbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geometry preview (group-level). Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping geometry preview cell (set RUN_TETRODE_CHECKS=True to run).')\n",
        "else:\n",
        "    # Channel geometry preview.\n",
        "\n",
        "    SHOW_LABELS = False\n",
        "    LABEL_EVERY = 2\n",
        "    LABEL_FONTSIZE = 6\n",
        "    COLOR_BY_GROUP = True\n",
        "    SHOW_ORIGINAL_LAYOUT = False  # set True to plot original group layout with gaps\n",
        "\n",
        "    positions, labels_file, groups, pos_path = _load_phy_geometry(phy_folder)\n",
        "\n",
        "    labels = None\n",
        "    if 'EXPORTED_CHANNEL_LABELS' in globals() and not isinstance(EXPORTED_CHANNEL_LABELS, str):\n",
        "        labels = EXPORTED_CHANNEL_LABELS\n",
        "    if labels is None:\n",
        "        labels = labels_file\n",
        "\n",
        "    if positions is not None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        if COLOR_BY_GROUP and groups is not None and groups.shape[0] == positions.shape[0]:\n",
        "            unique_groups = sorted({int(g) for g in groups.tolist()})\n",
        "            cmap = plt.get_cmap('tab20', max(len(unique_groups), 1))\n",
        "            colors = [cmap(unique_groups.index(int(g)) % cmap.N) for g in groups]\n",
        "            ax.scatter(positions[:, 0], positions[:, 1], s=50, c=colors)\n",
        "            handles = [\n",
        "                plt.Line2D([0], [0], marker='o', color='none', markerfacecolor=cmap(i), markersize=6, label=str(g))\n",
        "                for i, g in enumerate(unique_groups)\n",
        "            ]\n",
        "            ax.legend(handles=handles, title='group', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "        else:\n",
        "            ax.scatter(positions[:, 0], positions[:, 1], s=50)\n",
        "\n",
        "        if labels is not None and SHOW_LABELS:\n",
        "            for idx, ((x, y), label) in enumerate(zip(positions, labels)):\n",
        "                if LABEL_EVERY and idx % LABEL_EVERY != 0:\n",
        "                    continue\n",
        "                ax.text(x, y, str(label), fontsize=LABEL_FONTSIZE, ha='center', va='center')\n",
        "\n",
        "        x_min, x_max = float(np.min(positions[:, 0])), float(np.max(positions[:, 0]))\n",
        "        y_min, y_max = float(np.min(positions[:, 1])), float(np.max(positions[:, 1]))\n",
        "        pad = 0.05 * max(x_max - x_min, y_max - y_min, 1.0)\n",
        "        ax.set_xlim(x_min - pad, x_max + pad)\n",
        "        ax.set_ylim(y_min - pad, y_max + pad)\n",
        "\n",
        "        ax.set_title(f'Channel geometry ({pos_path.name})')\n",
        "        ax.set_xlabel('um')\n",
        "        ax.set_ylabel('um')\n",
        "        ax.set_aspect('equal', adjustable='box')\n",
        "        ax.invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print('channel_positions.npy Not Found; Export Geometry To Enable This Plot.')\n",
        "\n",
        "    if SHOW_ORIGINAL_LAYOUT:\n",
        "        required = [\n",
        "            phy_folder / 'channel_ids.npy',\n",
        "            phy_folder / 'channel_groups.npy',\n",
        "            phy_folder / 'channel_shanks.npy',\n",
        "        ]\n",
        "        if not all(p.exists() for p in required):\n",
        "            print('Original Layout Plot Requires channel_ids.npy, channel_groups.npy, channel_shanks.npy')\n",
        "        else:\n",
        "            ch_ids = np.load(phy_folder / 'channel_ids.npy')\n",
        "            ch_groups = np.load(phy_folder / 'channel_groups.npy')\n",
        "            unique_groups = sorted({int(g) for g in ch_groups.tolist()})\n",
        "            groups_per_row = max(1, int(np.ceil(np.sqrt(len(unique_groups)))))\n",
        "            dx = float(globals().get('TETRODE_SPACING_DX_UM', 300.0))\n",
        "            dy = float(globals().get('TETRODE_SPACING_DY_UM', 300.0))\n",
        "            pitch = float(globals().get('TETRODE_PITCH_UM', 20.0))\n",
        "            positions_full = _build_synthetic_positions(ch_groups, dx, dy, pitch)\n",
        "            if positions_full is None:\n",
        "                print('Original Layout Plot: No Groups Found.')\n",
        "                positions_full = np.zeros((ch_groups.size, 2), dtype=float)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(8, 8))\n",
        "            if COLOR_BY_GROUP:\n",
        "                cmap = plt.get_cmap('tab20', max(len(unique_groups), 1))\n",
        "                colors = [cmap(unique_groups.index(int(g)) % cmap.N) for g in ch_groups]\n",
        "                ax.scatter(positions_full[:, 0], positions_full[:, 1], s=50, c=colors)\n",
        "                handles = [\n",
        "                    plt.Line2D([0], [0], marker='o', color='none', markerfacecolor=cmap(i), markersize=6, label=str(g))\n",
        "                    for i, g in enumerate(unique_groups)\n",
        "                ]\n",
        "                ax.legend(handles=handles, title='group', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "            else:\n",
        "                ax.scatter(positions_full[:, 0], positions_full[:, 1], s=50)\n",
        "\n",
        "            if labels is not None and SHOW_LABELS:\n",
        "                for idx, ((x, y), label) in enumerate(zip(positions_full, labels)):\n",
        "                    if LABEL_EVERY and idx % LABEL_EVERY != 0:\n",
        "                        continue\n",
        "                    ax.text(x, y, str(label), fontsize=LABEL_FONTSIZE, ha='center', va='center')\n",
        "\n",
        "            ax.set_title('Original group layout (with gaps)')\n",
        "            ax.set_xlabel('um')\n",
        "            ax.set_ylabel('um')\n",
        "            ax.set_aspect('equal', adjustable='box')\n",
        "            ax.invert_yaxis()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5225be5",
      "metadata": {},
      "source": [
        "### Map exported channel_ids to Open Ephys labels\n",
        "\n",
        "If exported channel_ids are numeric they index Open Ephys channel order; if strings they already represent labels.\n",
        "Labels are derived from OE channel_name when available.\n",
        "Expected: a printed mapping from exported IDs to OE labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e25194da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map exported channel_ids to Open Ephys labels.\n",
        "\n",
        "data_path = globals().get('data_path')\n",
        "stream_name = globals().get('stream_name')\n",
        "oe_pick_index = globals().get('oe_pick_index')\n",
        "OE_PROMPT_SELECT = globals().get('OE_PROMPT_SELECT', False)\n",
        "\n",
        "if 'get_oe_recording' not in globals():\n",
        "    print('Run the resolve OE folder cell first to enable label checks.')\n",
        "    oe_recording = None\n",
        "    oe_ids = None\n",
        "    oe_names = None\n",
        "else:\n",
        "    oe_recording, oe_folder, stream_name, oe_ids, oe_names = get_oe_recording(\n",
        "        data_path, stream_name=stream_name, oe_pick_index=oe_pick_index, prompt=OE_PROMPT_SELECT\n",
        "    )\n",
        "\n",
        "if oe_recording is not None:\n",
        "    # Preview OE channel index -> name from structure.oebin.\n",
        "    if oe_ids is not None and oe_names is not None:\n",
        "        print('OE channel index -> name (first 12):')\n",
        "        for idx, name in list(zip(oe_ids, oe_names))[:12]:\n",
        "            print(f'  {idx} -> {name}')\n",
        "\n",
        "    # Optional: gain/unit sanity check.\n",
        "    try:\n",
        "        gains = oe_recording.get_channel_gains() if hasattr(oe_recording, 'get_channel_gains') else None\n",
        "    except Exception:\n",
        "        gains = None\n",
        "    try:\n",
        "        units = oe_recording.get_channel_units() if hasattr(oe_recording, 'get_channel_units') else None\n",
        "    except Exception:\n",
        "        units = None\n",
        "    gain_prop = False\n",
        "    try:\n",
        "        gain_prop = 'gain_to_uV' in oe_recording.get_property_keys()\n",
        "    except Exception:\n",
        "        gain_prop = False\n",
        "    print('gains present:', gains is not None, 'gain_to_uV prop:', gain_prop)\n",
        "    if gains is not None:\n",
        "        try:\n",
        "            print('gains[0:5]:', gains[:5])\n",
        "        except Exception:\n",
        "            print('gains:', gains)\n",
        "    if units is not None:\n",
        "        try:\n",
        "            print('units[0:5]:', units[:5])\n",
        "        except Exception:\n",
        "            print('units:', units)\n",
        "\n",
        "    ch_ids_path = phy_folder / 'channel_ids.npy'\n",
        "    if ch_ids_path.exists():\n",
        "        export_ids = np.load(ch_ids_path)\n",
        "        export_labels = [resolve_oe_label(raw_idx, oe_ids, oe_names) for raw_idx in export_ids]\n",
        "        EXPORTED_CHANNEL_LABELS = export_labels\n",
        "        print('Exported channel_ids -> OE labels (first 12):')\n",
        "        for raw_idx, label in zip(export_ids[:12], export_labels[:12]):\n",
        "            print(f'{raw_idx} = {label}')\n",
        "\n",
        "        # Full mapping table (Phy index -> export_id -> OE label).\n",
        "        try:\n",
        "            mapping_rows = []\n",
        "            for i, (raw_idx, label) in enumerate(zip(export_ids, export_labels)):\n",
        "                try:\n",
        "                    raw_val = raw_idx.item()\n",
        "                except Exception:\n",
        "                    raw_val = raw_idx\n",
        "                mapping_rows.append((i, str(raw_val), str(label)))\n",
        "\n",
        "            print(f'Exported channel_id mapping ({len(mapping_rows)} total):')\n",
        "            print('phy_idx | oe_index | oe_label')\n",
        "            for i, raw_val, label in mapping_rows:\n",
        "                print(f'{i:>3}: {raw_val} -> {label}')\n",
        "\n",
        "            WRITE_MAPPING_TSV = False  # set True to save mapping next to export\n",
        "            if WRITE_MAPPING_TSV:\n",
        "                tsv_path = phy_folder / 'channel_map_labels.tsv'\n",
        "                with tsv_path.open('w', encoding='utf-8') as f:\n",
        "                    f.write('phy_idx\\toe_index\\toe_label\\n')\n",
        "                    for i, raw_val, label in mapping_rows:\n",
        "                        f.write(f'{i}\\t{raw_val}\\t{label}\\n')\n",
        "                print(f'Wrote {tsv_path}')\n",
        "        except Exception as exc:\n",
        "            print(f'Channel mapping print skipped: {exc}')\n",
        "    else:\n",
        "        print(f'channel_ids.npy not found at {ch_ids_path}')\n",
        "else:\n",
        "    print('Set data_path to an Open Ephys folder to map channel_ids to labels.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfc5789",
      "metadata": {},
      "source": [
        "### Translate Phy channel IDs to OE labels\n",
        "\n",
        "Paste Phy channel indices (contiguous export order) and map them back to OE index/label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec165d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translate Phy channel indices (contiguous export order) to OE index/label.\n",
        "# Edit the list below.\n",
        "phy_indices = [0, 1, 2]\n",
        "\n",
        "def _get_export_mapping():\n",
        "    if \"export_ids\" in globals() and \"export_labels\" in globals():\n",
        "        ids = list(export_ids)\n",
        "        labels = list(export_labels)\n",
        "        return ids, labels\n",
        "\n",
        "    ch_ids_path = phy_folder / \"channel_ids.npy\"\n",
        "    if not ch_ids_path.exists():\n",
        "        print(f\"channel_ids.npy not found at {ch_ids_path}\")\n",
        "        return None, None\n",
        "\n",
        "    ids = list(np.load(ch_ids_path))\n",
        "    labels = []\n",
        "    if \"oe_ids\" in globals() and \"oe_names\" in globals():\n",
        "        labels = [resolve_oe_label(raw_idx, oe_ids, oe_names) for raw_idx in ids]\n",
        "    else:\n",
        "        labels = [str(raw_idx) for raw_idx in ids]\n",
        "    return ids, labels\n",
        "\n",
        "ids, labels = _get_export_mapping()\n",
        "if ids is None or labels is None:\n",
        "    print(\"Run the mapping cell above first.\")\n",
        "else:\n",
        "    for i in phy_indices:\n",
        "        try:\n",
        "            raw_idx = ids[i]\n",
        "            label = labels[i]\n",
        "            print(f\"phy_idx {i}: oe_index {raw_idx} | oe_label {label}\")\n",
        "        except Exception as exc:\n",
        "            print(f\"phy_idx {i}: lookup failed ({exc})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa770ae",
      "metadata": {},
      "source": [
        "### Group -> channel labels\n",
        "Expected: per-group channel label lists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d8b79f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group-level diagnostics. Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Diagnostics Cell (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "    # Show group -> channel labels.\n",
        "    data_path = globals().get('data_path')\n",
        "    stream_name = globals().get('stream_name')\n",
        "    oe_pick_index = globals().get('oe_pick_index')\n",
        "    OE_PROMPT_SELECT = globals().get('OE_PROMPT_SELECT', False)\n",
        "\n",
        "    if 'get_oe_recording' not in globals():\n",
        "        print('Run the resolve OE folder cell first to enable label checks.')\n",
        "        oe_ids = None\n",
        "        oe_names = None\n",
        "    else:\n",
        "        oe_recording, oe_folder, stream_name, oe_ids, oe_names = get_oe_recording(\n",
        "            data_path, stream_name=stream_name, oe_pick_index=oe_pick_index, prompt=OE_PROMPT_SELECT\n",
        "        )\n",
        "\n",
        "    if oe_ids is None:\n",
        "        print('Set data_path to enable OE label mapping.')\n",
        "    else:\n",
        "        groups_path = phy_folder / 'channel_groups.npy'\n",
        "        ch_ids_path = phy_folder / 'channel_ids.npy'\n",
        "        if groups_path.exists() and ch_ids_path.exists():\n",
        "            channel_groups = np.load(groups_path)\n",
        "            export_ids = np.load(ch_ids_path)\n",
        "            group_map = {}\n",
        "            for idx, grp in enumerate(channel_groups):\n",
        "                label = resolve_oe_label(export_ids[idx], oe_ids, oe_names)\n",
        "                group_map.setdefault(int(grp), []).append(label)\n",
        "            for grp in sorted(group_map):\n",
        "                print(f'group {grp}: {group_map[grp]}')\n",
        "        else:\n",
        "            print('Ensure channel_groups.npy/channel_ids.npy exist.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabe1bc9",
      "metadata": {},
      "source": [
        "### Available unit IDs\n",
        "Print unit IDs available in this export.\n",
        "Expected: a list of unit IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90222926",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Available unit IDs.\n",
        "spike_clusters_path = phy_folder / 'spike_clusters.npy'\n",
        "if spike_clusters_path.exists():\n",
        "    spike_clusters = np.load(spike_clusters_path).reshape(-1)\n",
        "    unit_ids = np.unique(spike_clusters)\n",
        "    print('n_units:', unit_ids.size)\n",
        "    print('unit_ids (first 50):', unit_ids[:50])\n",
        "else:\n",
        "    print('spike_clusters.npy Not Found; Run Export First.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fab13a1b",
      "metadata": {},
      "source": [
        "### Cluster group summary\n",
        "Count cluster_group.tsv labels (good/mua/unsorted/noise).\n",
        "Expected: counts per cluster group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ad3dfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster group summary.\n",
        "\n",
        "tsv = phy_folder / 'cluster_group.tsv'\n",
        "if not tsv.exists():\n",
        "    print('cluster_group.tsv Not Found')\n",
        "else:\n",
        "    counts = {}\n",
        "    with tsv.open('r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f, delimiter='\t')\n",
        "        for row in reader:\n",
        "            label = row.get('group', 'unknown')\n",
        "            counts[label] = counts.get(label, 0) + 1\n",
        "    print('Cluster group counts:')\n",
        "    for key in sorted(counts):\n",
        "        print(f'  {key}: {counts[key]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd8672f",
      "metadata": {},
      "source": [
        "### Units overview (counts + peak-channel groups)\n",
        "Quick summary per group from TSV and template peak channels.\n",
        "Expected: counts per group and template peak summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa85418",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group-level diagnostics. Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Diagnostics Cell (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "    # Units overview (counts + peak-channel groups).\n",
        "\n",
        "    spike_clusters_path = phy_folder / 'spike_clusters.npy'\n",
        "    tsv = phy_folder / 'cluster_channel_group.tsv'\n",
        "    channel_groups_path = phy_folder / 'channel_groups.npy'\n",
        "    spike_templates_path = phy_folder / 'spike_templates.npy'\n",
        "    templates_path = phy_folder / 'templates.npy'\n",
        "    template_ind_path = phy_folder / 'template_ind.npy'\n",
        "\n",
        "    if not spike_clusters_path.exists():\n",
        "        print('spike_clusters.npy Not Found; Run Export First.')\n",
        "    else:\n",
        "        spike_clusters = globals().get('spike_clusters')\n",
        "        unit_ids = globals().get('unit_ids')\n",
        "        if spike_clusters is None or unit_ids is None:\n",
        "            spike_clusters = np.load(spike_clusters_path).reshape(-1)\n",
        "            unit_ids = np.unique(spike_clusters)\n",
        "        print('n_units:', unit_ids.size)\n",
        "\n",
        "        # Per-group counts from cluster_channel_group.tsv.\n",
        "        if tsv.exists():\n",
        "            cluster_to_group = {}\n",
        "            with tsv.open('r', encoding='utf-8') as f:\n",
        "                reader = csv.DictReader(f, delimiter='\t')\n",
        "                for row in reader:\n",
        "                    try:\n",
        "                        cluster_id = int(row.get('cluster_id', -1))\n",
        "                        group_id = int(row.get('channel_group', row.get('group', -1)))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if cluster_id >= 0 and group_id >= 0:\n",
        "                        cluster_to_group[cluster_id] = group_id\n",
        "            if cluster_to_group:\n",
        "                group_counts = {}\n",
        "                for unit_id in unit_ids:\n",
        "                    gid = cluster_to_group.get(int(unit_id))\n",
        "                    if gid is None:\n",
        "                        continue\n",
        "                    group_counts[gid] = group_counts.get(gid, 0) + 1\n",
        "                print('Units Per Group (From cluster_channel_group.tsv):')\n",
        "                for gid in sorted(group_counts):\n",
        "                    print(f'  group {gid}: {group_counts[gid]}')\n",
        "            else:\n",
        "                print('cluster_channel_group.tsv Empty Or Missing Groups.')\n",
        "        else:\n",
        "            print('cluster_channel_group.tsv Not Found.')\n",
        "\n",
        "        # Per-group counts from template peak channels.\n",
        "        if channel_groups_path.exists() and spike_templates_path.exists() and templates_path.exists():\n",
        "            channel_groups = np.load(channel_groups_path)\n",
        "            spike_templates = np.load(spike_templates_path).reshape(-1).astype(int)\n",
        "            templates = np.load(templates_path, mmap_mode='r')\n",
        "            template_ind = np.load(template_ind_path, mmap_mode='r') if template_ind_path.exists() else None\n",
        "\n",
        "            # template_ind shape check.\n",
        "            if template_ind is not None:\n",
        "                if template_ind.ndim != 2 or template_ind.shape[0] != templates.shape[0]:\n",
        "                    print(f'Warning: template_ind shape {template_ind.shape} does not match templates {templates.shape}')\n",
        "\n",
        "            max_template = int(spike_templates.max(initial=0))\n",
        "            factor = max_template + 1\n",
        "            keys = spike_clusters.astype(np.int64) * factor + spike_templates.astype(np.int64)\n",
        "            uniq, counts = np.unique(keys, return_counts=True)\n",
        "            unit_to_template = {}\n",
        "            for key, cnt in zip(uniq, counts):\n",
        "                unit = int(key // factor)\n",
        "                template = int(key % factor)\n",
        "                prev = unit_to_template.get(unit)\n",
        "                if prev is None or cnt > prev[1]:\n",
        "                    unit_to_template[unit] = (template, cnt)\n",
        "\n",
        "            peak_group_counts = {}\n",
        "            for unit, (template, _) in unit_to_template.items():\n",
        "                if template < 0 or template >= templates.shape[0]:\n",
        "                    continue\n",
        "                ptp = np.ptp(templates[template], axis=0)\n",
        "                if ptp.size == 0:\n",
        "                    continue\n",
        "                peak_idx = int(np.argmax(ptp))\n",
        "                if template_ind is not None and template_ind.shape[0] > template and template_ind.shape[1] == ptp.size:\n",
        "                    peak_chan = int(template_ind[template, peak_idx])\n",
        "                else:\n",
        "                    peak_chan = peak_idx\n",
        "                if peak_chan < 0 or peak_chan >= len(channel_groups):\n",
        "                    continue\n",
        "                group_id = int(channel_groups[peak_chan])\n",
        "                peak_group_counts[group_id] = peak_group_counts.get(group_id, 0) + 1\n",
        "            if peak_group_counts:\n",
        "                print('Units Per Group (By Template Peak Channel):')\n",
        "                for gid in sorted(peak_group_counts):\n",
        "                    print(f'  group {gid}: {peak_group_counts[gid]}')\n",
        "            else:\n",
        "                print('Could Not Compute Peak-Channel Group Counts.')\n",
        "        else:\n",
        "            print('Peak-Channel Group Counts Skipped (Missing templates/channel_groups/spike_templates).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56f23b1",
      "metadata": {},
      "source": [
        "### Map unit -> group -> channels\n",
        "Expected: per-unit mapping of group and channel labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe85feb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group-level diagnostics. Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Diagnostics Cell (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "    # Map unit -> group -> channels.\n",
        "\n",
        "    UNIT_ID = 11  # set a unit id to inspect\n",
        "\n",
        "    tsv = phy_folder / \"cluster_channel_group.tsv\"\n",
        "    groups_path = phy_folder / \"channel_groups.npy\"\n",
        "    ch_ids_path = phy_folder / \"channel_ids.npy\"\n",
        "\n",
        "    if not tsv.exists():\n",
        "        print(\"cluster_channel_group.tsv Not Found\")\n",
        "    elif not groups_path.exists() or not ch_ids_path.exists():\n",
        "        print(\"channel_groups.npy Or channel_ids.npy Not Found\")\n",
        "    else:\n",
        "        group_id = None\n",
        "        with tsv.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f, delimiter=\"\t\")\n",
        "            for row in reader:\n",
        "                try:\n",
        "                    if int(row.get(\"cluster_id\", -1)) == UNIT_ID:\n",
        "                        group_id = int(row.get(\"channel_group\", -1))\n",
        "                        break\n",
        "                except Exception:\n",
        "                    continue\n",
        "        if group_id is None:\n",
        "            print(f\"Unit {UNIT_ID} Not Found In cluster_channel_group.tsv\")\n",
        "        else:\n",
        "            channel_groups = np.load(groups_path)\n",
        "            export_ids = np.load(ch_ids_path)\n",
        "            labels = None\n",
        "            if \"EXPORTED_CHANNEL_LABELS\" in globals():\n",
        "                labels = EXPORTED_CHANNEL_LABELS\n",
        "            if labels is None:\n",
        "                # Fall back to OE labels if available.\n",
        "                labels = []\n",
        "                for raw_idx in export_ids:\n",
        "                    try:\n",
        "                        idx = int(raw_idx)\n",
        "                    except (TypeError, ValueError):\n",
        "                        labels.append(str(raw_idx))\n",
        "                        continue\n",
        "                    label = None\n",
        "                    if \"oe_names\" in globals() and oe_names is not None and 0 <= idx < len(oe_names):\n",
        "                        label = oe_names[idx]\n",
        "                    elif \"oe_ids\" in globals() and oe_ids is not None and 0 <= idx < len(oe_ids):\n",
        "                        label = oe_ids[idx]\n",
        "                    else:\n",
        "                        label = str(raw_idx)\n",
        "                    labels.append(label)\n",
        "            channels = [labels[i] for i in np.where(channel_groups == group_id)[0]]\n",
        "            print(f\"unit {UNIT_ID} -> group {group_id}\")\n",
        "            print(\"channels:\", channels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5728a80d",
      "metadata": {},
      "source": [
        "### Check unit group vs template peak channel\n",
        "Compare each unit's group to its template peak-channel group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0330bb0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group-level diagnostics. Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Diagnostics Cell (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "    # Check unit group assignment vs template peak channel.\n",
        "\n",
        "    CHECK_ALL_UNITS = True  # set False to check a single unit\n",
        "    UNIT_ID = 11  # used when CHECK_ALL_UNITS=False\n",
        "    MAX_REPORT = 20  # max mismatches to print when CHECK_ALL_UNITS=True\n",
        "\n",
        "    tsv = phy_folder / \"cluster_channel_group.tsv\"\n",
        "    channel_groups_path = phy_folder / \"channel_groups.npy\"\n",
        "    channel_ids_path = phy_folder / \"channel_ids.npy\"\n",
        "    spike_clusters_path = phy_folder / \"spike_clusters.npy\"\n",
        "    spike_templates_path = phy_folder / \"spike_templates.npy\"\n",
        "    templates_path = phy_folder / \"templates.npy\"\n",
        "    template_ind_path = phy_folder / \"template_ind.npy\"\n",
        "    cluster_info_path = phy_folder / \"cluster_info.tsv\"\n",
        "\n",
        "    def _load_vec(path):\n",
        "        arr = np.load(path)\n",
        "        return arr.reshape(-1)\n",
        "\n",
        "    def _load_cluster_groups(tsv_path):\n",
        "        cluster_to_group = {}\n",
        "        if not tsv_path.exists():\n",
        "            return cluster_to_group\n",
        "        with tsv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f, delimiter=\"\t\")\n",
        "            for row in reader:\n",
        "                try:\n",
        "                    cluster_id = int(row.get(\"cluster_id\", -1))\n",
        "                except Exception:\n",
        "                    continue\n",
        "                try:\n",
        "                    group_id = int(row.get(\"channel_group\", row.get(\"group\", -1)))\n",
        "                except Exception:\n",
        "                    group_id = -1\n",
        "                if cluster_id >= 0 and group_id >= 0:\n",
        "                    cluster_to_group[cluster_id] = group_id\n",
        "        return cluster_to_group\n",
        "\n",
        "    def _map_labels(export_ids, oe_names=None, oe_ids=None):\n",
        "        labels = []\n",
        "        for raw_idx in export_ids:\n",
        "            try:\n",
        "                idx = int(raw_idx)\n",
        "            except (TypeError, ValueError):\n",
        "                labels.append(str(raw_idx))\n",
        "                continue\n",
        "            label = None\n",
        "            if oe_names is not None and 0 <= idx < len(oe_names):\n",
        "                label = oe_names[idx]\n",
        "            elif oe_ids is not None and 0 <= idx < len(oe_ids):\n",
        "                label = oe_ids[idx]\n",
        "            else:\n",
        "                label = str(raw_idx)\n",
        "            labels.append(label)\n",
        "        return labels\n",
        "\n",
        "    def _load_template_map(spike_clusters, spike_templates):\n",
        "        max_template = int(spike_templates.max(initial=0))\n",
        "        factor = max_template + 1\n",
        "        keys = spike_clusters.astype(np.int64) * factor + spike_templates.astype(np.int64)\n",
        "        uniq, counts = np.unique(keys, return_counts=True)\n",
        "        best = {}\n",
        "        for key, cnt in zip(uniq, counts):\n",
        "            cluster = int(key // factor)\n",
        "            template = int(key % factor)\n",
        "            prev = best.get(cluster)\n",
        "            if prev is None or cnt > prev[1]:\n",
        "                best[cluster] = (template, cnt)\n",
        "        return {k: v[0] for k, v in best.items()}\n",
        "\n",
        "    def _peak_channel_for_template(template_id, templates, template_ind):\n",
        "        if templates is None or template_id is None:\n",
        "            return None\n",
        "        if template_id < 0 or template_id >= templates.shape[0]:\n",
        "            return None\n",
        "        ptp = np.ptp(templates[template_id], axis=0)\n",
        "        if ptp.size == 0:\n",
        "            return None\n",
        "        peak_idx = int(np.argmax(ptp))\n",
        "        if template_ind is not None and template_id < template_ind.shape[0]:\n",
        "            if template_ind.shape[1] == ptp.size:\n",
        "                return int(template_ind[template_id, peak_idx])\n",
        "        return peak_idx\n",
        "\n",
        "    if not channel_groups_path.exists() or not channel_ids_path.exists():\n",
        "        print(\"channel_groups.npy Or channel_ids.npy Not Found\")\n",
        "    else:\n",
        "        channel_groups = np.load(channel_groups_path)\n",
        "        export_ids = np.load(channel_ids_path)\n",
        "        labels = None\n",
        "        if \"EXPORTED_CHANNEL_LABELS\" in globals():\n",
        "            labels = EXPORTED_CHANNEL_LABELS\n",
        "        if labels is None:\n",
        "            labels = _map_labels(export_ids, globals().get('oe_names'), globals().get('oe_ids'))\n",
        "\n",
        "        cluster_to_group = _load_cluster_groups(tsv)\n",
        "        if not cluster_to_group:\n",
        "            print(\"cluster_channel_group.tsv Not Found or empty\")\n",
        "        else:\n",
        "            templates = np.load(templates_path, mmap_mode='r') if templates_path.exists() else None\n",
        "            template_ind = np.load(template_ind_path, mmap_mode='r') if template_ind_path.exists() else None\n",
        "            template_map = {}\n",
        "\n",
        "            if spike_clusters_path.exists() and spike_templates_path.exists():\n",
        "                spike_clusters = _load_vec(spike_clusters_path)\n",
        "                spike_templates = _load_vec(spike_templates_path)\n",
        "                if CHECK_ALL_UNITS:\n",
        "                    template_map = _load_template_map(spike_clusters, spike_templates)\n",
        "                else:\n",
        "                    mask = spike_clusters == UNIT_ID\n",
        "                    if np.any(mask):\n",
        "                        templates_for_unit = spike_templates[mask].astype(int)\n",
        "                        template_map[UNIT_ID] = int(np.bincount(templates_for_unit).argmax())\n",
        "            elif cluster_info_path.exists():\n",
        "                with cluster_info_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                    reader = csv.DictReader(f, delimiter=\"\t\")\n",
        "                    for row in reader:\n",
        "                        try:\n",
        "                            cluster_id = int(row.get(\"cluster_id\", -1))\n",
        "                            if \"template\" in row:\n",
        "                                template_map[cluster_id] = int(row.get(\"template\"))\n",
        "                        except Exception:\n",
        "                            continue\n",
        "\n",
        "            if CHECK_ALL_UNITS:\n",
        "                mismatches = []\n",
        "                for unit_id, group_id in sorted(cluster_to_group.items()):\n",
        "                    template_id = template_map.get(unit_id)\n",
        "                    peak_chan = _peak_channel_for_template(template_id, templates, template_ind)\n",
        "                    if peak_chan is None:\n",
        "                        continue\n",
        "                    peak_group = int(channel_groups[peak_chan])\n",
        "                    peak_label = labels[peak_chan] if peak_chan < len(labels) else peak_chan\n",
        "                    if group_id != peak_group:\n",
        "                        mismatches.append((unit_id, group_id, peak_group, peak_label))\n",
        "                if mismatches:\n",
        "                    print(f\"Group mismatches: {len(mismatches)}\")\n",
        "                    for row in mismatches[:MAX_REPORT]:\n",
        "                        unit_id, group_id, peak_group, peak_label = row\n",
        "                        print(f\"unit {unit_id}: tsv group {group_id}, peak {peak_label} -> group {peak_group}\")\n",
        "                    if len(mismatches) > MAX_REPORT:\n",
        "                        print(f\"... {len(mismatches) - MAX_REPORT} more\")\n",
        "                else:\n",
        "                    print(\"No group mismatches found (based on template peak channel).\")\n",
        "            else:\n",
        "                group_id = cluster_to_group.get(UNIT_ID)\n",
        "                template_id = template_map.get(UNIT_ID)\n",
        "                peak_chan = _peak_channel_for_template(template_id, templates, template_ind)\n",
        "                if group_id is None:\n",
        "                    print(f\"Unit {UNIT_ID} Not Found In cluster_channel_group.tsv\")\n",
        "                elif peak_chan is None:\n",
        "                    print(f\"Unit {UNIT_ID}: cannot determine peak channel\")\n",
        "                else:\n",
        "                    peak_group = int(channel_groups[peak_chan])\n",
        "                    peak_label = labels[peak_chan] if peak_chan < len(labels) else peak_chan\n",
        "                    print(f\"Unit {UNIT_ID}: tsv group {group_id}, peak {peak_label} -> group {peak_group}\")\n",
        "                    if group_id != peak_group:\n",
        "                        print(\"Mismatch: unit group assignment may be wrong for this unit.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0c0bd4",
      "metadata": {},
      "source": [
        "### Inspect exported recording.dat\n",
        "Expected: file size/shape summary (or missing-file notice).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d7929f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect exported recording.dat.\n",
        "dat_path = phy_folder / 'recording.dat'\n",
        "params_path = phy_folder / 'params.py'\n",
        "\n",
        "if dat_path.exists():\n",
        "    ch_ids = np.load(phy_folder / 'channel_ids.npy')\n",
        "    n_channels = int(ch_ids.size)\n",
        "\n",
        "    dtype = np.float32\n",
        "    if params_path.exists():\n",
        "        try:\n",
        "            params = runpy.run_path(params_path)\n",
        "            dtype = np.dtype(params.get('dtype', 'float32'))\n",
        "        except Exception as exc:\n",
        "            print(f'Warning: could not parse dtype from params.py ({exc}); using float32')\n",
        "\n",
        "    mm = np.memmap(dat_path, dtype=dtype, mode='r')\n",
        "    n_samples = mm.size // n_channels\n",
        "    window = min(30000, n_samples)\n",
        "    X = mm[:n_channels * window].reshape(window, n_channels)\n",
        "    print('Channels:', n_channels, 'Dtype:', dtype, 'Samples:', X.shape[0])\n",
        "    print('per-chan std (median):', float(np.median(X.std(axis=0))))\n",
        "    print('per-chan p2p (median):', float(np.median(np.ptp(X, axis=0))))\n",
        "    print('global min/max:', float(X.min()), float(X.max()))\n",
        "    frac_gt500 = float(np.sum(np.abs(X) > 500) / X.size)\n",
        "    print('fraction |value| > 500 uV:', frac_gt500)\n",
        "else:\n",
        "    print('recording.dat Not Found; Run Export First.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb275c44",
      "metadata": {},
      "source": [
        "### Compare Open Ephys vs exported .dat\n",
        "Expected: alignment stats and basic sanity checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9885b553",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Open Ephys vs exported .dat.\n",
        "data_path = globals().get('data_path')\n",
        "stream_name = globals().get('stream_name')\n",
        "oe_pick_index = globals().get('oe_pick_index')\n",
        "OE_PROMPT_SELECT = globals().get('OE_PROMPT_SELECT', False)\n",
        "\n",
        "if 'get_oe_recording' not in globals():\n",
        "    print('Run the resolve OE folder cell first to enable OE/DAT comparison.')\n",
        "    oe_recording = None\n",
        "else:\n",
        "    oe_recording, oe_folder, stream_name, oe_ids, oe_names = get_oe_recording(\n",
        "        data_path, stream_name=stream_name, oe_pick_index=oe_pick_index, prompt=OE_PROMPT_SELECT\n",
        "    )\n",
        "\n",
        "if oe_recording is not None:\n",
        "    fs = oe_recording.get_sampling_frequency()\n",
        "    X_scaled = get_traces_scaled(oe_recording, start_frame=0, end_frame=int(fs))\n",
        "    print('OE scaled  median std:', np.median(X_scaled.std(axis=0)))\n",
        "    print('OE scaled  median p2p:', np.median(np.ptp(X_scaled, axis=0)))\n",
        "    print('OE scaled  min/max:', X_scaled.min(), X_scaled.max())\n",
        "\n",
        "    dat_path = phy_folder / 'recording.dat'\n",
        "    params_path = phy_folder / 'params.py'\n",
        "    if dat_path.exists():\n",
        "        ch_ids = np.load(phy_folder / 'channel_ids.npy')\n",
        "        n_channels = int(ch_ids.size)\n",
        "\n",
        "        dtype = np.float32\n",
        "        if params_path.exists():\n",
        "            try:\n",
        "                params = runpy.run_path(params_path)\n",
        "                dtype = np.dtype(params.get('dtype', 'float32'))\n",
        "            except Exception as exc:\n",
        "                print(f'Warning: could not parse dtype from params.py ({exc}); using float32')\n",
        "\n",
        "        mm = np.memmap(dat_path, dtype=dtype, mode='r')\n",
        "        print('DAT dtype:', mm.dtype)\n",
        "        X_dat = mm[:n_channels * int(fs)].reshape(int(fs), n_channels)\n",
        "        print('DAT median std:', np.median(X_dat.std(axis=0)))\n",
        "        print('DAT median p2p:', np.median(np.ptp(X_dat, axis=0)))\n",
        "        print('DAT min/max:', X_dat.min(), X_dat.max())\n",
        "    else:\n",
        "        print('recording.dat Not Found; Run Export First.')\n",
        "else:\n",
        "    print('Set data_path to your Open Ephys folder to run this check.')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c623d48",
      "metadata": {},
      "source": [
        "### QC with SortingAnalyzer\n",
        "Expected: QC extension outputs and plots (if enabled).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3dcf4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# QC diagnostics (SortingAnalyzer). Skip when RUN_TETRODE_CHECKS is False.\n",
        "if SKIP_NOISY_CELLS or not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping QC diagnostics cell (set RUN_TETRODE_CHECKS=True to run).')\n",
        "else:\n",
        "    # QC with SortingAnalyzer (templates/PCs/correlograms/metrics).\n",
        "    from spikeinterface.extractors import read_phy\n",
        "    from spikeinterface.core import create_sorting_analyzer\n",
        "    from spikeinterface.extractors.extractor_classes import BinaryRecordingExtractor\n",
        "\n",
        "    if 'PROJECT_ROOT' in globals():\n",
        "        _proj_root = PROJECT_ROOT\n",
        "    else:\n",
        "        _proj_root = Path.cwd().resolve().parent\n",
        "    if str(_proj_root) not in sys.path:\n",
        "        sys.path.insert(0, str(_proj_root))\n",
        "\n",
        "    RUN_QC = True  # set True to build analyzer + compute QC extensions\n",
        "    PLOT_QC = True  # set True to display QC plots in the notebook\n",
        "    QC_SPARSE = False  # set True to estimate sparsity (probe required; use False for correct group labels)\n",
        "    MIN_SPIKES_QC = 100  # skip QC metrics for units with fewer spikes (set None to disable)\n",
        "    AMP_CUTOFF_MIN_SPIKES = 2500  # 500 bins * ratio 5; avoid amplitude_cutoff NaNs\n",
        "    UNIT_INDEX = 46  # which unit to plot (picked for visible amplitudes)\n",
        "    PAIR_INDEX = 1  # second unit for cross-correlograms\n",
        "    PLOT_WF_TIME_AXIS = True  # show waveform grid with time in ms\n",
        "    WF_CHANNELS = 4  # max channels to plot per unit (mixed mode may map units to 2-ch or 4-ch groups)\n",
        "    WF_MAX_SPIKES = 80  # smaller subset for cleaner overlay\n",
        "    WF_ALPHA = 0.05\n",
        "    WF_USE_UNIT_GROUP = False  # use top-PTP channels for clearer waveforms\n",
        "    WF_GROUP_MIN_FRAC = 0.4  # only use group channels if strong enough\n",
        "\n",
        "    if RUN_QC:\n",
        "        params = runpy.run_path(phy_folder / 'params.py')\n",
        "        dat_path = Path(params['dat_path'])\n",
        "        n_channels = int(params.get('n_channels_dat', params.get('n_channels')))\n",
        "        sample_rate = float(params.get('sample_rate', params.get('sample_rate_dat', 30000.0)))\n",
        "        dtype = np.dtype(params.get('dtype', 'float32'))\n",
        "\n",
        "        sorting = read_phy(phy_folder)\n",
        "        sorting_qc = sorting\n",
        "        spike_counts_qc = None\n",
        "        if MIN_SPIKES_QC is not None and MIN_SPIKES_QC > 0:\n",
        "            if hasattr(sorting, 'count_num_spikes_per_unit'):\n",
        "                spike_counts = sorting.count_num_spikes_per_unit()\n",
        "                keep_units = [uid for uid, count in spike_counts.items() if int(count) >= MIN_SPIKES_QC]\n",
        "                if keep_units:\n",
        "                    if len(keep_units) < len(sorting.unit_ids):\n",
        "                        print(f\"QC: keeping {len(keep_units)} / {len(sorting.unit_ids)} units with >= {MIN_SPIKES_QC} spikes\")\n",
        "                    spike_counts_qc = {uid: spike_counts[uid] for uid in keep_units if uid in spike_counts}\n",
        "                    sorting_qc = sorting.select_units(keep_units)\n",
        "                else:\n",
        "                    print(f\"QC: no units meet MIN_SPIKES_QC={MIN_SPIKES_QC}; keeping all units\")\n",
        "            else:\n",
        "                print(\"QC: spike count filter not available; keeping all units\")\n",
        "        if spike_counts_qc is None and hasattr(sorting_qc, 'count_num_spikes_per_unit'):\n",
        "            try:\n",
        "                spike_counts_qc = sorting_qc.count_num_spikes_per_unit()\n",
        "            except Exception:\n",
        "                spike_counts_qc = None\n",
        "        try:\n",
        "            recording = BinaryRecordingExtractor(\n",
        "                file_paths=[dat_path],\n",
        "                sampling_frequency=sample_rate,\n",
        "                num_channels=n_channels,\n",
        "                dtype=dtype,\n",
        "            )\n",
        "        except TypeError:\n",
        "            recording = BinaryRecordingExtractor(\n",
        "                file_path=dat_path,\n",
        "                sampling_frequency=sample_rate,\n",
        "                num_channels=n_channels,\n",
        "                dtype=dtype,\n",
        "            )\n",
        "\n",
        "        export_labels_for_qc = None\n",
        "        try:\n",
        "            if 'EXPORTED_CHANNEL_LABELS' in globals():\n",
        "                export_labels_for_qc = EXPORTED_CHANNEL_LABELS\n",
        "        except Exception:\n",
        "            export_labels_for_qc = None\n",
        "        if export_labels_for_qc is None:\n",
        "            ch_ids_path = phy_folder / 'channel_ids.npy'\n",
        "            if ch_ids_path.exists():\n",
        "                try:\n",
        "                    export_ids = np.load(ch_ids_path)\n",
        "                    export_labels_for_qc = []\n",
        "                    for raw_idx in export_ids:\n",
        "                        try:\n",
        "                            idx = int(raw_idx)\n",
        "                        except (TypeError, ValueError):\n",
        "                            export_labels_for_qc.append(str(raw_idx))\n",
        "                            continue\n",
        "                        label = None\n",
        "                        if 'oe_names' in globals() and oe_names is not None and 0 <= idx < len(oe_names):\n",
        "                            label = oe_names[idx]\n",
        "                        elif 'oe_ids' in globals() and oe_ids is not None and 0 <= idx < len(oe_ids):\n",
        "                            label = oe_ids[idx]\n",
        "                        else:\n",
        "                            label = str(raw_idx)\n",
        "                        export_labels_for_qc.append(label)\n",
        "                except Exception:\n",
        "                    export_labels_for_qc = None\n",
        "        EXPORT_LABELS_FOR_QC = export_labels_for_qc\n",
        "        positions, _, channel_groups_for_geom, pos_path = _load_phy_geometry(phy_folder)\n",
        "        if positions is None:\n",
        "            if channel_groups_for_geom is not None:\n",
        "                dx = float(globals().get('TETRODE_SPACING_DX_UM', 300.0))\n",
        "                dy = float(globals().get('TETRODE_SPACING_DY_UM', 300.0))\n",
        "                pitch = float(globals().get('TETRODE_PITCH_UM', 20.0))\n",
        "                positions = _build_synthetic_positions(channel_groups_for_geom, dx, dy, pitch)\n",
        "                if positions is not None:\n",
        "                    print('Warning: channel_positions.npy Not Found; Using Synthetic Group Geometry For QC Plots.')\n",
        "                else:\n",
        "                    print('Warning: channel_positions.npy Not Found; Skipping Probe Attach (QC Sparsity/Plots May Be Limited).')\n",
        "            else:\n",
        "                print('Warning: channel_positions.npy Not Found; Skipping Probe Attach (QC Sparsity/Plots May Be Limited).')\n",
        "\n",
        "        if positions is not None:\n",
        "            try:\n",
        "                rec_loc = recording.set_channel_locations(positions, in_place=False)\n",
        "            except TypeError:\n",
        "                rec_loc = recording.set_channel_locations(positions)\n",
        "            if rec_loc is not None:\n",
        "                recording = rec_loc\n",
        "\n",
        "        probe_attached = False\n",
        "        if positions is not None:\n",
        "            try:\n",
        "                from probeinterface import Probe\n",
        "                locs = recording.get_channel_locations()\n",
        "                probe = Probe(ndim=locs.shape[1])\n",
        "                probe.set_contacts(positions=locs, shapes='circle', shape_params={'radius': 5})\n",
        "                probe.set_device_channel_indices(np.arange(recording.get_num_channels()))\n",
        "                recording = recording.set_probe(probe, in_place=False)\n",
        "                probe_attached = recording.get_probe() is not None\n",
        "                print('Attached probe from channel locations for QC.')\n",
        "            except Exception as exc:\n",
        "                print(f'Warning: could not attach probe for QC analyzer: {exc}')\n",
        "\n",
        "        if QC_SPARSE and not probe_attached:\n",
        "            print('QC_SPARSE disabled because no probe is attached.')\n",
        "            QC_SPARSE = False\n",
        "\n",
        "        print('QC recording probe attached:', probe_attached)\n",
        "\n",
        "        # Keep export recording explicit for downstream curation/export cells.\n",
        "        export_recording = recording\n",
        "\n",
        "        analyzer = create_sorting_analyzer(\n",
        "            sorting=sorting_qc,\n",
        "            recording=recording,\n",
        "            folder=phy_folder / 'analyzer_qc',\n",
        "            overwrite=True,\n",
        "            sparse=QC_SPARSE,\n",
        "        )\n",
        "        analyzer.compute('random_spikes')\n",
        "        analyzer.compute('waveforms')\n",
        "        analyzer.compute('templates')\n",
        "        analyzer.compute('correlograms')\n",
        "        try:\n",
        "            analyzer.compute('noise_levels')\n",
        "        except Exception as exc:\n",
        "            print(f'Noise levels not available: {exc}')\n",
        "        print('QC analyzer ready:', analyzer.folder if getattr(analyzer, 'folder', None) else 'in-memory')\n",
        "\n",
        "        try:\n",
        "            import spikeinterface.qualitymetrics as sqm\n",
        "            desired_metrics = [\n",
        "                'snr',\n",
        "                'isi_violation',\n",
        "                'firing_rate',\n",
        "                'presence_ratio',\n",
        "                'amplitude_cutoff',\n",
        "            ]\n",
        "            available = None\n",
        "            for attr in ('get_quality_metric_list', 'get_quality_metrics_list'):\n",
        "                func = getattr(sqm, attr, None)\n",
        "                if callable(func):\n",
        "                    try:\n",
        "                        available = func()\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        available = None\n",
        "            if available:\n",
        "                metric_names = [m for m in desired_metrics if m in available]\n",
        "                missing = [m for m in desired_metrics if m not in metric_names]\n",
        "                if missing:\n",
        "                    print('Skipping unavailable metrics:', missing)\n",
        "            else:\n",
        "                metric_names = desired_metrics\n",
        "            if metric_names:\n",
        "                if 'amplitude_cutoff' in metric_names and spike_counts_qc:\n",
        "                    try:\n",
        "                        min_spikes = min(int(c) for c in spike_counts_qc.values())\n",
        "                    except Exception:\n",
        "                        min_spikes = None\n",
        "                    if min_spikes is not None and min_spikes < AMP_CUTOFF_MIN_SPIKES:\n",
        "                        metric_names = [m for m in metric_names if m != 'amplitude_cutoff']\n",
        "                        print(f\"Skipping amplitude_cutoff (min spikes {min_spikes} < {AMP_CUTOFF_MIN_SPIKES}).\")\n",
        "                metrics = sqm.compute_quality_metrics(\n",
        "                    analyzer,\n",
        "                    metric_names=metric_names,\n",
        "                )\n",
        "                print(metrics.head())\n",
        "            else:\n",
        "                print('No quality metrics available for this SI version.')\n",
        "        except Exception as exc:\n",
        "            print(f'Quality metrics not available: {exc}')\n",
        "\n",
        "        if PLOT_QC:\n",
        "            unit_ids = analyzer.sorting.unit_ids\n",
        "            if unit_ids.size == 0:\n",
        "                print('No units to plot.')\n",
        "            else:\n",
        "                unit_id = unit_ids[min(UNIT_INDEX, unit_ids.size - 1)]\n",
        "                unit_group_id = None\n",
        "                group_channels = None\n",
        "                if WF_USE_UNIT_GROUP:\n",
        "                    try:\n",
        "                        tsv_path = phy_folder / \"cluster_channel_group.tsv\"\n",
        "                        groups_path = phy_folder / \"channel_groups.npy\"\n",
        "                        if tsv_path.exists() and groups_path.exists():\n",
        "                            unit_group_id = None\n",
        "                            with tsv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                                reader = csv.DictReader(f, delimiter=\"\t\")\n",
        "                                for row in reader:\n",
        "                                    try:\n",
        "                                        if int(row.get(\"cluster_id\", -1)) == int(unit_id):\n",
        "                                            unit_group_id = int(row.get(\"channel_group\", -1))\n",
        "                                            break\n",
        "                                    except Exception:\n",
        "                                        continue\n",
        "                            if unit_group_id is not None:\n",
        "                                channel_groups = np.load(groups_path)\n",
        "                                group_channels = np.where(channel_groups == unit_group_id)[0]\n",
        "                                if group_channels.size > 0:\n",
        "                                    labels = None\n",
        "                                    if \"EXPORT_LABELS_FOR_QC\" in globals():\n",
        "                                        labels = EXPORT_LABELS_FOR_QC\n",
        "                                    if labels is None:\n",
        "                                        labels = recording.channel_ids\n",
        "                                    chan_labels = [labels[i] for i in group_channels]\n",
        "                                    print(f\"Unit {unit_id} -> group {unit_group_id} channels: {chan_labels}\")\n",
        "                    except Exception as exc:\n",
        "                        print(f\"Warning: unit-group mapping failed: {exc}\")\n",
        "                unit_sparse_channels = None\n",
        "                try:\n",
        "                    sparsity = getattr(analyzer, \"sparsity\", None)\n",
        "                    if sparsity is not None:\n",
        "                        unit_sparse_channels = sparsity.unit_id_to_channel_indices.get(unit_id)\n",
        "                        if unit_sparse_channels is None:\n",
        "                            try:\n",
        "                                unit_sparse_channels = sparsity.unit_id_to_channel_indices.get(int(unit_id))\n",
        "                            except Exception:\n",
        "                                unit_sparse_channels = None\n",
        "                except Exception:\n",
        "                    unit_sparse_channels = None\n",
        "\n",
        "                if unit_sparse_channels is not None:\n",
        "                    if (not WF_USE_UNIT_GROUP) or (group_channels is None):\n",
        "                        labels = None\n",
        "                        if \"EXPORT_LABELS_FOR_QC\" in globals():\n",
        "                            labels = EXPORT_LABELS_FOR_QC\n",
        "                        if labels is None:\n",
        "                            labels = recording.channel_ids\n",
        "                        chan_labels = [labels[i] for i in unit_sparse_channels]\n",
        "                        print(f\"QC sparsity channels: {chan_labels}\")\n",
        "                if PLOT_WF_TIME_AXIS:\n",
        "                    try:\n",
        "                        wf_ext = analyzer.get_extension('waveforms')\n",
        "                        waveforms = None\n",
        "                        wf_channel_map = None\n",
        "                        if QC_SPARSE and unit_sparse_channels is not None:\n",
        "                            try:\n",
        "                                waveforms = wf_ext.get_waveforms_one_unit(unit_id)\n",
        "                                wf_channel_map = np.array(unit_sparse_channels)\n",
        "                            except Exception:\n",
        "                                waveforms = None\n",
        "                                wf_channel_map = None\n",
        "                        if waveforms is None:\n",
        "                            try:\n",
        "                                waveforms = wf_ext.get_waveforms_one_unit(unit_id, force_dense=True)\n",
        "                            except TypeError:\n",
        "                                waveforms = wf_ext.get_waveforms_one_unit(unit_id)\n",
        "                            if waveforms is None:\n",
        "                                raise RuntimeError('No waveforms returned for unit')\n",
        "                            wf_channel_map = np.arange(waveforms.shape[2])\n",
        "                        if waveforms.size == 0:\n",
        "                            print(f'No waveforms available for unit {unit_id}.')\n",
        "                        else:\n",
        "                            n_spikes, n_samples, n_channels = waveforms.shape\n",
        "                            nbefore = getattr(wf_ext, 'nbefore', None)\n",
        "                            if nbefore is None:\n",
        "                                ms_before = getattr(wf_ext, '_params', {}).get('ms_before', 1.0)\n",
        "                                nbefore = int(round(ms_before * sample_rate / 1000.0))\n",
        "                            t_ms = (np.arange(n_samples) - nbefore) / sample_rate * 1000.0\n",
        "                            mean_wf = waveforms.mean(axis=0)\n",
        "                            ptp = np.ptp(mean_wf, axis=0)\n",
        "                            ptp_all_max = np.nanmax(ptp) if np.isfinite(ptp).any() else 0.0\n",
        "\n",
        "                            labels = None\n",
        "                            if 'EXPORT_LABELS_FOR_QC' in globals():\n",
        "                                labels = EXPORT_LABELS_FOR_QC\n",
        "                            if labels is None:\n",
        "                                labels = recording.channel_ids\n",
        "\n",
        "                            group_local = None\n",
        "                            if group_channels is not None and group_channels.size > 0:\n",
        "                                group_local = np.nonzero(np.isin(wf_channel_map, group_channels))[0]\n",
        "                                if group_local.size == 0:\n",
        "                                    group_local = None\n",
        "                                    if QC_SPARSE:\n",
        "                                        print('Group Channels Not In Waveform Map; Set QC_SPARSE=False To Plot Group Channels.')\n",
        "\n",
        "                            sparse_local = None\n",
        "                            if unit_sparse_channels is not None:\n",
        "                                sparse_local = np.nonzero(np.isin(wf_channel_map, unit_sparse_channels))[0]\n",
        "                                if sparse_local.size == 0:\n",
        "                                    sparse_local = None\n",
        "\n",
        "                            top_local = None\n",
        "                            source = 'ptp'\n",
        "                            if group_local is not None:\n",
        "                                ptp_group = ptp[group_local]\n",
        "                                ptp_group_max = np.nanmax(ptp_group) if np.isfinite(ptp_group).any() else 0.0\n",
        "                                if ptp_all_max > 0 and ptp_group_max >= WF_GROUP_MIN_FRAC * ptp_all_max:\n",
        "                                    top_local = group_local\n",
        "                                    source = 'group'\n",
        "                                    if top_local.size > WF_CHANNELS:\n",
        "                                        top_local = top_local[np.argsort(ptp_group)[::-1][:WF_CHANNELS]]\n",
        "                            if top_local is None and sparse_local is not None:\n",
        "                                top_local = sparse_local\n",
        "                                source = 'sparse'\n",
        "                                if top_local.size > WF_CHANNELS:\n",
        "                                    ptp_sparse = ptp[top_local]\n",
        "                                    top_local = top_local[np.argsort(ptp_sparse)[::-1][:WF_CHANNELS]]\n",
        "                            if top_local is None:\n",
        "                                top_n = min(WF_CHANNELS, n_channels)\n",
        "                                top_local = np.argsort(ptp)[::-1][:top_n]\n",
        "\n",
        "                            top_global = np.asarray(wf_channel_map)[top_local]\n",
        "                            try:\n",
        "                                print(f\"Waveform channels source: {source} -> {[labels[i] for i in top_global]}\")\n",
        "                            except Exception:\n",
        "                                print(f\"Waveform channels source: {source}\")\n",
        "\n",
        "                            try:\n",
        "                                locs = recording.get_channel_locations()\n",
        "                                loc_sel = locs[top_global]\n",
        "                                order = np.lexsort((loc_sel[:, 0], loc_sel[:, 1]))\n",
        "                                top_local = top_local[order]\n",
        "                                top_global = top_global[order]\n",
        "                            except Exception:\n",
        "                                pass\n",
        "\n",
        "                            if WF_MAX_SPIKES and n_spikes > WF_MAX_SPIKES:\n",
        "                                idx = np.random.choice(n_spikes, size=WF_MAX_SPIKES, replace=False)\n",
        "                                wf_plot = waveforms[idx]\n",
        "                            else:\n",
        "                                wf_plot = waveforms\n",
        "                            ncols = 2\n",
        "                            nrows = int(np.ceil(top_local.size / ncols))\n",
        "                            fig, axes = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(6, 4))\n",
        "                            axes = np.atleast_1d(axes).reshape(nrows, ncols)\n",
        "                            for j, ch_local in enumerate(top_local):\n",
        "                                ax = axes[j // ncols, j % ncols]\n",
        "                                ax.plot(t_ms, wf_plot[:, :, ch_local].T, color='0.8', alpha=WF_ALPHA, linewidth=0.5)\n",
        "                                ax.plot(t_ms, mean_wf[:, ch_local], color='tab:blue', linewidth=1.5)\n",
        "                                ch_label = None\n",
        "                                try:\n",
        "                                    if labels is not None and top_global[j] < len(labels):\n",
        "                                        ch_label = labels[top_global[j]]\n",
        "                                except Exception:\n",
        "                                    ch_label = None\n",
        "                                if ch_label is None:\n",
        "                                    ch_label = recording.channel_ids[int(top_global[j])]\n",
        "                                ax.set_title(f\"{ch_label}\")\n",
        "                            for j in range(top_local.size, nrows * ncols):\n",
        "                                axes[j // ncols, j % ncols].axis('off')\n",
        "                            axes[-1, 0].set_xlabel('Time (ms)')\n",
        "                            axes[-1, 0].set_ylabel('Amplitude (uV if export scaled)')\n",
        "                            fig.suptitle(f\"Unit {unit_id} waveforms ({source}, top {top_local.size})\")\n",
        "                            plt.show()\n",
        "                    except Exception as exc:\n",
        "                        print(f\"Manual waveform plot failed: {exc}\")\n",
        "\n",
        "                sw.plot_autocorrelograms(analyzer, unit_ids=[unit_id], window_ms=50, bin_ms=1)\n",
        "                if unit_ids.size > 1:\n",
        "                    unit_id_2 = unit_ids[min(PAIR_INDEX, unit_ids.size - 1)]\n",
        "                    sw.plot_crosscorrelograms(\n",
        "                        analyzer,\n",
        "                        unit_ids=[unit_id, unit_id_2],\n",
        "                        window_ms=50,\n",
        "                        bin_ms=1,\n",
        "                        min_similarity_for_correlograms=0,\n",
        "                    )\n",
        "    else:\n",
        "        print('Set RUN_QC=True to build a SortingAnalyzer and compute QC extensions.')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d23bbb",
      "metadata": {},
      "source": [
        "## QC + auto-curation\n",
        "This section assumes a SortingAnalyzer is loaded (e.g., `analyzer_sc2`).\n",
        "Run the analyzer load cell (or set `analyzer` manually) before QC/curation.\n",
        "Expected: QC tables and optional curated export when enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d28f6f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# QC metrics (requires SortingAnalyzer).\n",
        "RUN_QC_METRICS = False\n",
        "QC_METRIC_NAMES = [\n",
        "    \"firing_rate\",\n",
        "    \"presence_ratio\",\n",
        "    \"isi_violation\",\n",
        "    \"snr\",\n",
        "    \"amplitude_cutoff\",\n",
        "]\n",
        "\n",
        "if not RUN_QC_METRICS:\n",
        "    print(\"QC metrics skipped; set RUN_QC_METRICS=True to run.\")\n",
        "else:\n",
        "    try:\n",
        "        from spikeinterface.qualitymetrics import compute_quality_metrics\n",
        "    except Exception as exc:\n",
        "        compute_quality_metrics = None\n",
        "        print(f\"qualitymetrics not available: {exc}\")\n",
        "\n",
        "    analyzer_obj = globals().get(\"analyzer_sc2\") or globals().get(\"analyzer\")\n",
        "    if analyzer_obj is None:\n",
        "        print(\"No analyzer found; load one above or run the pipeline first.\")\n",
        "    elif compute_quality_metrics is None:\n",
        "        print(\"qualitymetrics not available; skipping QC metrics.\")\n",
        "    else:\n",
        "        qm = compute_quality_metrics(analyzer_obj, metric_names=QC_METRIC_NAMES)\n",
        "        print(\"QC metrics computed:\")\n",
        "        display(qm.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ebd8a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-curation (optional): filter units by QC metrics.\n",
        "RUN_AUTO_CURATION = False\n",
        "AUTO_CURATION_THRESHOLDS = {\n",
        "    'snr_min': 5.0,\n",
        "    'isi_violation_max': 0.5,\n",
        "    'amplitude_cutoff_max': 0.1,\n",
        "    'presence_ratio_min': 0.9,\n",
        "    'firing_rate_min': 0.1,\n",
        "}\n",
        "\n",
        "if not RUN_AUTO_CURATION:\n",
        "    print('Auto-curation skipped; set RUN_AUTO_CURATION=True to run.')\n",
        "else:\n",
        "    metrics = globals().get('qm')\n",
        "    analyzer_obj = globals().get('analyzer_sc2') or globals().get('analyzer')\n",
        "    if metrics is None:\n",
        "        print('Auto-Curation Skipped: QC Metrics Not Found (Run QC Cell First).')\n",
        "    elif analyzer_obj is None:\n",
        "        print('Auto-curation skipped: no analyzer available.')\n",
        "    else:\n",
        "        df = metrics.copy()\n",
        "        def _col(name):\n",
        "            for c in df.columns:\n",
        "                if c == name:\n",
        "                    return c\n",
        "            return None\n",
        "        snr_c = _col('snr')\n",
        "        isi_c = _col('isi_violation')\n",
        "        amp_c = _col('amplitude_cutoff')\n",
        "        pr_c = _col('presence_ratio')\n",
        "        fr_c = _col('firing_rate')\n",
        "\n",
        "        mask = np.ones(len(df), dtype=bool)\n",
        "        if snr_c is not None:\n",
        "            mask &= df[snr_c].fillna(-np.inf) >= AUTO_CURATION_THRESHOLDS['snr_min']\n",
        "        if isi_c is not None:\n",
        "            mask &= df[isi_c].fillna(np.inf) <= AUTO_CURATION_THRESHOLDS['isi_violation_max']\n",
        "        if amp_c is not None:\n",
        "            mask &= df[amp_c].fillna(np.inf) <= AUTO_CURATION_THRESHOLDS['amplitude_cutoff_max']\n",
        "        if pr_c is not None:\n",
        "            mask &= df[pr_c].fillna(-np.inf) >= AUTO_CURATION_THRESHOLDS['presence_ratio_min']\n",
        "        if fr_c is not None:\n",
        "            mask &= df[fr_c].fillna(-np.inf) >= AUTO_CURATION_THRESHOLDS['firing_rate_min']\n",
        "\n",
        "        keep_ids = df.index[mask].tolist()\n",
        "        print(f'Auto-curation keeps {len(keep_ids)} / {len(df)} units.')\n",
        "        curated_analyzer = analyzer_obj.select_units(keep_ids)\n",
        "        print('Curated analyzer stored as: curated_analyzer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed4324e",
      "metadata": {},
      "source": [
        "### Automated curation\n",
        "Filter units based on QC metrics and export curated sorting.\n",
        "Expected: curated unit count and export folder (if enabled).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904edaf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automated curation.\n",
        "from datetime import datetime\n",
        "from spikeinterface.exporters import export_to_phy\n",
        "\n",
        "qm = globals().get(\"qm\", None)\n",
        "if qm is None:\n",
        "    qm = globals().get(\"metrics\", None)\n",
        "\n",
        "curation_sorting = globals().get('sorting')\n",
        "curation_recording = globals().get('export_recording')\n",
        "\n",
        "if qm is None or curation_sorting is None or curation_recording is None:\n",
        "    print(\"Quality metrics or sorting/export_recording not available for curation. Run QC cell first.\")\n",
        "else:\n",
        "    # Define thresholds.\n",
        "    snr_thresh = 2.0\n",
        "    isi_thresh = 0.4\n",
        "    amp_cutoff_thresh = 0.2\n",
        "\n",
        "    snr_col = \"snr\" if \"snr\" in qm.columns else None\n",
        "    isi_col = \"isi_violations_ratio\" if \"isi_violations_ratio\" in qm.columns else (\n",
        "        \"isi_violation\" if \"isi_violation\" in qm.columns else None\n",
        "    )\n",
        "    amp_col = \"amplitude_cutoff\" if \"amplitude_cutoff\" in qm.columns else None\n",
        "\n",
        "    missing = [\n",
        "        name for name, col in ((\"snr\", snr_col), (\"isi\", isi_col), (\"amplitude_cutoff\", amp_col))\n",
        "        if col is None\n",
        "    ]\n",
        "    if missing:\n",
        "        print(f\"Missing metrics columns: {missing}; skipping automated curation.\")\n",
        "    else:\n",
        "        good_units = qm[\n",
        "            (qm[snr_col] > snr_thresh) &\n",
        "            (qm[isi_col] < isi_thresh) &\n",
        "            (qm[amp_col] < amp_cutoff_thresh)\n",
        "        ].index.tolist()\n",
        "\n",
        "        print(f\"Good units: {len(good_units)} out of {len(qm)}\")\n",
        "\n",
        "        if good_units:\n",
        "            curated_sorting = curation_sorting.select_units(good_units)\n",
        "            curated_folder = phy_folder.parent / f\"phy_curated_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "            export_to_phy(\n",
        "                curated_sorting,\n",
        "                recording=curation_recording,\n",
        "                output_folder=curated_folder,\n",
        "                remove_if_exists=True,\n",
        "                copy_binary=True,\n",
        "                verbose=False,\n",
        "            )\n",
        "            print(f\"Curated sorting exported to {curated_folder}\")\n",
        "        else:\n",
        "            print(\"No units passed thresholds; skipping export.\")\n",
        "\n",
        "print(\"Automated curation complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e0c6d4",
      "metadata": {},
      "source": [
        "### Interactive elements\n",
        "Browse units and adjust thresholds interactively.\n",
        "Expected: widgets and interactive plots (if ipywidgets is available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77b08bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive elements (requires ipywidgets).\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    print(\"ipywidgets not installed; skipping interactive elements.\")\n",
        "else:\n",
        "    qm = globals().get(\"qm\", None)\n",
        "    if qm is None:\n",
        "        qm = globals().get(\"metrics\", None)\n",
        "\n",
        "    if 'sorting' not in locals() or 'analyzer' not in locals():\n",
        "        print(\"Run QC cell first to enable interactive elements.\")\n",
        "    else:\n",
        "        unit_options = [f\"Unit {u}\" for u in sorting.unit_ids]\n",
        "        if not unit_options:\n",
        "            print(\"No units available for interactive plots.\")\n",
        "        else:\n",
        "            snr_slider = widgets.FloatSlider(\n",
        "                value=2.0, min=0.5, max=10.0, step=0.1, description='SNR Threshold:'\n",
        "            )\n",
        "            unit_dropdown = widgets.Dropdown(options=unit_options, description='Select Unit:')\n",
        "\n",
        "            def update_plot(snr_thresh, selected_unit):\n",
        "                unit_id = int(selected_unit.split()[1])\n",
        "                if qm is not None and 'snr' in qm.columns:\n",
        "                    good = qm['snr'] > snr_thresh\n",
        "                    print(f\"Units above SNR {snr_thresh}: {good.sum()}\")\n",
        "                sw.plot_unit_waveforms(analyzer, unit_ids=[unit_id])\n",
        "                plt.show()\n",
        "\n",
        "            interactive_plot = widgets.interactive(\n",
        "                update_plot, snr_thresh=snr_slider, selected_unit=unit_dropdown\n",
        "            )\n",
        "            display(interactive_plot)\n",
        "\n",
        "print(\"Interactive elements loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47eb3cd9",
      "metadata": {},
      "source": [
        "### Additional visualizations\n",
        "Waveforms, ISI distributions, and correlograms.\n",
        "Expected: waveform/ISI/correlogram plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ae3912",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional visualizations.\n",
        "\n",
        "if 'analyzer' not in locals() or 'sorting' not in locals():\n",
        "    print(\"Run QC cell first to enable additional visualizations.\")\n",
        "else:\n",
        "    unit_ids = sorting.unit_ids[:5]\n",
        "    if unit_ids.size == 0:\n",
        "        print(\"No units available for waveforms/ISI/correlograms.\")\n",
        "    else:\n",
        "        sw.plot_unit_waveforms(analyzer, unit_ids=unit_ids)\n",
        "        plt.show()\n",
        "        sw.plot_isi_distribution(analyzer, unit_ids=unit_ids, bin_ms=1.0)\n",
        "        plt.show()\n",
        "        try:\n",
        "            sw.plot_crosscorrelograms(\n",
        "                analyzer,\n",
        "                unit_ids=unit_ids,\n",
        "                bin_ms=1.0,\n",
        "                min_similarity_for_correlograms=0,\n",
        "            )\n",
        "            plt.show()\n",
        "        except Exception as exc:\n",
        "            print(f\"Crosscorrelograms skipped: {exc}\")\n",
        "\n",
        "print(\"Additional visualizations complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba3ee40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot units by group with firing rates.\n",
        "if not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Plot (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "\n",
        "    qm = globals().get(\"qm\", None)\n",
        "    if qm is None:\n",
        "        qm = globals().get(\"metrics\", None)\n",
        "\n",
        "    if qm is None or 'sorting' not in locals():\n",
        "        print(\"Quality metrics or sorting not available for group firing rate plot.\")\n",
        "    elif 'firing_rate' not in qm.columns:\n",
        "        print(\"firing_rate metric not available; run QC with firing_rate enabled.\")\n",
        "    else:\n",
        "        tsv_path = phy_folder / \"cluster_channel_group.tsv\"\n",
        "        if not tsv_path.exists():\n",
        "            print(\"cluster_channel_group.tsv Not Found; skipping group firing rate plot.\")\n",
        "        else:\n",
        "            unit_to_group = {}\n",
        "            with tsv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f, delimiter=\"\t\")\n",
        "                for row in reader:\n",
        "                    try:\n",
        "                        unit_to_group[int(row.get(\"cluster_id\", -1))] = int(row.get(\"channel_group\", -1))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "            if not unit_to_group:\n",
        "                print(\"No unit->group mapping found; skipping.\")\n",
        "            else:\n",
        "                group_rates = {}\n",
        "                for unit in sorting.unit_ids:\n",
        "                    uid = int(unit)\n",
        "                    group = unit_to_group.get(uid, None)\n",
        "                    if group is None:\n",
        "                        continue\n",
        "                    if uid in qm.index:\n",
        "                        rate = qm.loc[uid, 'firing_rate']\n",
        "                    else:\n",
        "                        continue\n",
        "                    group_rates.setdefault(group, []).append(rate)\n",
        "\n",
        "                if not group_rates:\n",
        "                    print(\"No firing rates available for plotting.\")\n",
        "                else:\n",
        "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                    group_ids = sorted(group_rates.keys())\n",
        "                    for gid in group_ids:\n",
        "                        rates = group_rates[gid]\n",
        "                        ax.scatter([gid] * len(rates), rates, label=f'Group {gid}', alpha=0.7)\n",
        "                    ax.set_xlabel('Group')\n",
        "                    ax.set_ylabel('Firing Rate (Hz)')\n",
        "                    ax.set_title('Firing Rates by Group')\n",
        "                    ax.legend()\n",
        "                    plt.show()\n",
        "\n",
        "    print(\"Group firing rate plot complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e6a184",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot unit locations colored by group.\n",
        "if not RUN_TETRODE_CHECKS:\n",
        "    print('Skipping Group-Level Plot (Set RUN_TETRODE_CHECKS=True To Run).')\n",
        "else:\n",
        "\n",
        "    if 'analyzer' not in locals():\n",
        "        print(\"Run QC cell first to enable unit location plot.\")\n",
        "    else:\n",
        "        try:\n",
        "            sw.plot_unit_locations(analyzer, color='group')\n",
        "            plt.title('Unit Locations Colored by Group')\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not plot unit locations: {e}\")\n",
        "\n",
        "    print(\"Unit locations plot complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "175e3f74",
      "metadata": {},
      "source": [
        "### Troubleshooting\n",
        "Common issues and quick fixes for post-sort inspection.\n",
        "\n",
        "- **No units detected**: Check data quality and detection thresholds.\n",
        "- **High ISI violations**: Units may be over-merged; consider stricter thresholds or manual curation.\n",
        "- **Bad channel artifacts**: Update bad channel lists and re-run.\n",
        "- **Export errors**: Verify Phy install and file permissions.\n",
        "- **Low SNR**: Improve preprocessing (CAR/filtering) or electrode quality.\n",
        "\n",
        "For more help, check SpikeInterface docs or GitHub issues.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spikeinterface",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
